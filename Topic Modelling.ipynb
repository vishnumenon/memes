{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import itertools\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO\n",
    "\n",
    "def head(stream, n=10):\n",
    "    \"\"\" Return first n elements of \"\"\"\n",
    "    return list(itertools.islice(stream, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import json\n",
    "import io\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_comment_dump(dump_file):\n",
    "    \"\"\" Yield each comment from a json comment dump, as a `(subreddit, tokens)` 2-tuple.\"\"\"\n",
    "    for line in open(dump_file):\n",
    "        comment = json.loads(line)\n",
    "        tokens = tokenize(comment[\"body\"])\n",
    "        if len(tokens) < 3:\n",
    "            continue\n",
    "        yield comment[\"subreddit\"], tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdviceAnimals [u'thanks', u'obama', u'responding']\n",
      "SRSsucks [u'literally', u'assertion', u'true', u'true', u'women', u'created', u'patriarchy', u'hypothetical', u'way', u'literally']\n",
      "worldnews [u'budget', u'cuts', u'thanks', u'obama']\n",
      "Marvel [u'wow', u'going', u'restrain', u'clicking', u'night', u'thanks', u'obama', u'edit', u'clicked', u'questions']\n",
      "Buttcoin [u'gt', u'thanks', u'obama', u'welcome', u'barrack', u'obama']\n",
      "pics [u'baby', u'know', u'fucks', u'cookie', u'bake', u'shit', u'hope', u'hundreds', u'kids', u'flooding']\n",
      "nba [u'oh', u'thanks', u'obama']\n",
      "AskReddit [u'remember', u'super', u'secret', u'private', u'article', u'year', u'businesses', u'use', u'obamacare', u'excuse']\n",
      "politics [u'important', u'thing', u'know', u'want', u'corporate', u'interests', u'shine', u'foreign', u'policy', u'domestic']\n",
      "hearthstone [u'yeh', u'tried', u'play', u'yugioh', u'cards', u'drew', u'crayon', u'wouldnt', u'let', u'play']\n"
     ]
    }
   ],
   "source": [
    "comment_stream = iter_comment_dump('./all_years/obama-train.json')\n",
    "for sub, tokens in head(comment_stream):\n",
    "    print sub, tokens[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : adding document #10000 to Dictionary(22618 unique tokens: [u'deferment', u'woods', u'spiders', u'hanging', u'woody']...)\n",
      "INFO : adding document #20000 to Dictionary(32295 unique tokens: [u'deferment', u'woods', u'spiders', u'hanging', u'woody']...)\n",
      "INFO : built Dictionary(35902 unique tokens: [u'deferment', u'woods', u'spiders', u'hanging', u'woody']...) from 25154 documents (total 450294 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.76 s, sys: 182 ms, total: 3.94 s\n",
      "Wall time: 4.06 s\n",
      "Dictionary(35902 unique tokens: [u'deferment', u'woods', u'spiders', u'hanging', u'woody']...)\n"
     ]
    }
   ],
   "source": [
    "doc_stream = (tokens for _, tokens in comment_stream)\n",
    "%time id2word_comments = gensim.corpora.Dictionary(doc_stream)\n",
    "print(id2word_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : discarding 2 tokens: [(u'thanks', 25145), (u'obama', 24783)]...\n",
      "INFO : keeping 35900 tokens which were in no less than 0 and no more than 20123 (=80.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(35900 unique tokens: [u'deferment', u'askew', u'woods', u'spiders', u'hanging']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(35900 unique tokens: [u'deferment', u'askew', u'woods', u'spiders', u'hanging']...)\n"
     ]
    }
   ],
   "source": [
    "id2word_comments.filter_extremes(no_below=0, no_above=0.8)\n",
    "print(id2word_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11467, 1)]\n"
     ]
    }
   ],
   "source": [
    "class CommentCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip=None):\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip = clip\n",
    "    def __iter__(self):\n",
    "        self.subreddits = []\n",
    "        for sub, tokens in head(iter_comment_dump(self.dump_file), self.clip):\n",
    "            self.subreddits.append(sub)\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    def __len__(self):\n",
    "        return self.clip\n",
    "\n",
    "comment_corpus = CommentCorpus('./all_years/obama-train.json', id2word_comments)\n",
    "vector = next(iter(comment_corpus))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to ./topic_models/obama_bow.mm\n",
      "INFO : saving sparse matrix to ./topic_models/obama_bow.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : PROGRESS: saving document #5000\n",
      "INFO : PROGRESS: saving document #6000\n",
      "INFO : PROGRESS: saving document #7000\n",
      "INFO : PROGRESS: saving document #8000\n",
      "INFO : PROGRESS: saving document #9000\n",
      "INFO : PROGRESS: saving document #10000\n",
      "INFO : PROGRESS: saving document #11000\n",
      "INFO : PROGRESS: saving document #12000\n",
      "INFO : PROGRESS: saving document #13000\n",
      "INFO : PROGRESS: saving document #14000\n",
      "INFO : PROGRESS: saving document #15000\n",
      "INFO : PROGRESS: saving document #16000\n",
      "INFO : PROGRESS: saving document #17000\n",
      "INFO : PROGRESS: saving document #18000\n",
      "INFO : PROGRESS: saving document #19000\n",
      "INFO : PROGRESS: saving document #20000\n",
      "INFO : PROGRESS: saving document #21000\n",
      "INFO : PROGRESS: saving document #22000\n",
      "INFO : PROGRESS: saving document #23000\n",
      "INFO : PROGRESS: saving document #24000\n",
      "INFO : PROGRESS: saving document #25000\n",
      "INFO : saved 25164x35900 matrix, density=0.037% (332620/903387600)\n",
      "INFO : saving MmCorpus index to ./topic_models/obama_bow.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.78 s, sys: 272 ms, total: 5.05 s\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%time gensim.corpora.MmCorpus.serialize('./topic_models/obama_bow.mm', comment_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loaded corpus index from ./topic_models/obama_bow.mm.index\n",
      "INFO : initializing corpus reader from ./topic_models/obama_bow.mm\n",
      "INFO : accepted corpus with 25164 documents, 35900 features, 332620 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(25164 documents, 35900 features, 332620 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "mm_corpus = gensim.corpora.MmCorpus('./topic_models/obama_bow.mm')\n",
    "print(mm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11467, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "vector = next(iter(mm_corpus))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.1\n",
      "INFO : using symmetric eta at 2.78551532033e-05\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 10 topics, 4 passes over the supplied corpus of 4000 documents, updating model once every 2000 documents, evaluating perplexity every 4000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : PROGRESS: pass 0, at document #2000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #3 (0.100): 0.019*\"gt\" + 0.009*\"welcome\" + 0.008*\"barrack\" + 0.006*\"like\" + 0.004*\"griff\" + 0.004*\"marty\" + 0.004*\"com\" + 0.004*\"think\" + 0.003*\"http\" + 0.003*\"work\"\n",
      "INFO : topic #7 (0.100): 0.013*\"com\" + 0.013*\"http\" + 0.009*\"www\" + 0.008*\"comments\" + 0.008*\"reddit\" + 0.007*\"funny\" + 0.006*\"people\" + 0.006*\"like\" + 0.005*\"self\" + 0.004*\"think\"\n",
      "INFO : topic #4 (0.100): 0.009*\"com\" + 0.008*\"http\" + 0.007*\"people\" + 0.006*\"www\" + 0.006*\"like\" + 0.005*\"comments\" + 0.005*\"know\" + 0.005*\"reddit\" + 0.004*\"gt\" + 0.004*\"funny\"\n",
      "INFO : topic #5 (0.100): 0.009*\"com\" + 0.008*\"www\" + 0.006*\"http\" + 0.006*\"reddit\" + 0.005*\"comments\" + 0.004*\"think\" + 0.003*\"like\" + 0.003*\"time\" + 0.003*\"great\" + 0.003*\"funny\"\n",
      "INFO : topic #1 (0.100): 0.012*\"reddit\" + 0.010*\"http\" + 0.010*\"www\" + 0.009*\"com\" + 0.008*\"comments\" + 0.006*\"like\" + 0.005*\"gifs\" + 0.005*\"funny\" + 0.005*\"yr\" + 0.003*\"time\"\n",
      "INFO : topic diff=7.707792, rho=1.000000\n",
      "INFO : -11.882 per-word bound, 3773.5 perplexity estimate based on a held-out corpus of 2000 documents with 31745 words\n",
      "INFO : PROGRESS: pass 0, at document #4000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #8 (0.100): 0.007*\"people\" + 0.005*\"gt\" + 0.005*\"like\" + 0.004*\"ve\" + 0.004*\"insurance\" + 0.003*\"government\" + 0.003*\"right\" + 0.003*\"wow\" + 0.003*\"going\" + 0.003*\"work\"\n",
      "INFO : topic #0 (0.100): 0.011*\"reddit\" + 0.011*\"com\" + 0.010*\"bitcoin\" + 0.009*\"www\" + 0.009*\"http\" + 0.008*\"comments\" + 0.003*\"time\" + 0.003*\"link\" + 0.003*\"bot\" + 0.003*\"thread\"\n",
      "INFO : topic #2 (0.100): 0.009*\"like\" + 0.006*\"http\" + 0.005*\"com\" + 0.004*\"people\" + 0.004*\"edit\" + 0.003*\"imgur\" + 0.003*\"gif\" + 0.003*\"shit\" + 0.003*\"got\" + 0.003*\"blame\"\n",
      "INFO : topic #4 (0.100): 0.010*\"people\" + 0.006*\"like\" + 0.005*\"com\" + 0.005*\"know\" + 0.005*\"http\" + 0.005*\"want\" + 0.004*\"good\" + 0.004*\"way\" + 0.004*\"gt\" + 0.004*\"time\"\n",
      "INFO : topic #6 (0.100): 0.008*\"insurance\" + 0.007*\"time\" + 0.006*\"year\" + 0.005*\"like\" + 0.005*\"people\" + 0.004*\"ve\" + 0.003*\"pay\" + 0.003*\"thing\" + 0.003*\"got\" + 0.003*\"fucking\"\n",
      "INFO : topic diff=0.772387, rho=0.707107\n",
      "INFO : PROGRESS: pass 1, at document #2000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #3 (0.100): 0.027*\"gt\" + 0.014*\"welcome\" + 0.011*\"barrack\" + 0.005*\"like\" + 0.005*\"meme\" + 0.004*\"mean\" + 0.004*\"com\" + 0.004*\"work\" + 0.004*\"http\" + 0.004*\"griff\"\n",
      "INFO : topic #0 (0.100): 0.010*\"com\" + 0.010*\"reddit\" + 0.008*\"www\" + 0.007*\"http\" + 0.006*\"comments\" + 0.006*\"bitcoin\" + 0.003*\"bot\" + 0.003*\"time\" + 0.002*\"government\" + 0.002*\"thread\"\n",
      "INFO : topic #1 (0.100): 0.011*\"reddit\" + 0.009*\"www\" + 0.009*\"http\" + 0.008*\"com\" + 0.008*\"comments\" + 0.005*\"like\" + 0.004*\"gifs\" + 0.004*\"funny\" + 0.004*\"yr\" + 0.004*\"yeah\"\n",
      "INFO : topic #4 (0.100): 0.009*\"people\" + 0.007*\"like\" + 0.006*\"know\" + 0.004*\"way\" + 0.004*\"want\" + 0.004*\"good\" + 0.004*\"http\" + 0.004*\"think\" + 0.004*\"com\" + 0.004*\"things\"\n",
      "INFO : topic #2 (0.100): 0.008*\"like\" + 0.006*\"http\" + 0.004*\"com\" + 0.004*\"people\" + 0.003*\"edit\" + 0.003*\"imgur\" + 0.003*\"got\" + 0.003*\"blame\" + 0.003*\"gif\" + 0.003*\"shit\"\n",
      "INFO : topic diff=0.270614, rho=0.500000\n",
      "INFO : -9.984 per-word bound, 1012.6 perplexity estimate based on a held-out corpus of 2000 documents with 31745 words\n",
      "INFO : PROGRESS: pass 1, at document #4000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #0 (0.100): 0.013*\"com\" + 0.012*\"reddit\" + 0.011*\"bitcoin\" + 0.010*\"www\" + 0.010*\"http\" + 0.009*\"comments\" + 0.005*\"link\" + 0.003*\"bot\" + 0.003*\"thread\" + 0.003*\"amp\"\n",
      "INFO : topic #7 (0.100): 0.020*\"http\" + 0.019*\"com\" + 0.014*\"www\" + 0.013*\"reddit\" + 0.013*\"comments\" + 0.011*\"funny\" + 0.006*\"gifs\" + 0.006*\"yr\" + 0.006*\"people\" + 0.006*\"like\"\n",
      "INFO : topic #6 (0.100): 0.011*\"insurance\" + 0.008*\"time\" + 0.006*\"year\" + 0.005*\"like\" + 0.004*\"people\" + 0.004*\"pay\" + 0.004*\"ve\" + 0.004*\"health\" + 0.004*\"care\" + 0.003*\"thing\"\n",
      "INFO : topic #2 (0.100): 0.008*\"like\" + 0.005*\"http\" + 0.004*\"com\" + 0.004*\"edit\" + 0.004*\"people\" + 0.003*\"imgur\" + 0.003*\"gif\" + 0.003*\"shit\" + 0.003*\"blame\" + 0.003*\"oh\"\n",
      "INFO : topic #5 (0.100): 0.004*\"bad\" + 0.004*\"think\" + 0.003*\"great\" + 0.003*\"edit\" + 0.003*\"time\" + 0.003*\"word\" + 0.002*\"http\" + 0.002*\"com\" + 0.002*\"training\" + 0.002*\"like\"\n",
      "INFO : topic diff=0.239905, rho=0.500000\n",
      "INFO : PROGRESS: pass 2, at document #2000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #6 (0.100): 0.013*\"insurance\" + 0.008*\"time\" + 0.006*\"year\" + 0.005*\"like\" + 0.005*\"pay\" + 0.005*\"health\" + 0.004*\"care\" + 0.004*\"ve\" + 0.004*\"thing\" + 0.004*\"people\"\n",
      "INFO : topic #9 (0.100): 0.007*\"like\" + 0.007*\"people\" + 0.004*\"bot\" + 0.004*\"know\" + 0.004*\"got\" + 0.004*\"better\" + 0.004*\"years\" + 0.003*\"country\" + 0.003*\"way\" + 0.003*\"obamacare\"\n",
      "INFO : topic #5 (0.100): 0.004*\"great\" + 0.004*\"bad\" + 0.003*\"think\" + 0.003*\"edit\" + 0.003*\"time\" + 0.002*\"http\" + 0.002*\"word\" + 0.002*\"com\" + 0.002*\"news\" + 0.002*\"ll\"\n",
      "INFO : topic #0 (0.100): 0.011*\"com\" + 0.011*\"reddit\" + 0.009*\"www\" + 0.008*\"http\" + 0.007*\"comments\" + 0.007*\"bitcoin\" + 0.003*\"bot\" + 0.003*\"link\" + 0.003*\"government\" + 0.003*\"https\"\n",
      "INFO : topic #2 (0.100): 0.008*\"like\" + 0.004*\"http\" + 0.004*\"com\" + 0.003*\"edit\" + 0.003*\"people\" + 0.003*\"shit\" + 0.003*\"imgur\" + 0.003*\"blame\" + 0.003*\"gif\" + 0.003*\"got\"\n",
      "INFO : topic diff=0.181028, rho=0.447214\n",
      "INFO : -9.719 per-word bound, 843.0 perplexity estimate based on a held-out corpus of 2000 documents with 31745 words\n",
      "INFO : PROGRESS: pass 2, at document #4000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #5 (0.100): 0.004*\"bad\" + 0.003*\"great\" + 0.003*\"think\" + 0.003*\"edit\" + 0.003*\"time\" + 0.002*\"sarcastic\" + 0.002*\"training\" + 0.002*\"word\" + 0.002*\"news\" + 0.002*\"change\"\n",
      "INFO : topic #6 (0.100): 0.013*\"insurance\" + 0.008*\"time\" + 0.006*\"year\" + 0.005*\"health\" + 0.005*\"like\" + 0.005*\"care\" + 0.004*\"pay\" + 0.004*\"ve\" + 0.004*\"people\" + 0.003*\"got\"\n",
      "INFO : topic #9 (0.100): 0.008*\"people\" + 0.007*\"like\" + 0.005*\"bot\" + 0.004*\"better\" + 0.004*\"got\" + 0.004*\"years\" + 0.004*\"know\" + 0.003*\"need\" + 0.003*\"country\" + 0.003*\"think\"\n",
      "INFO : topic #0 (0.100): 0.013*\"com\" + 0.012*\"reddit\" + 0.011*\"bitcoin\" + 0.011*\"http\" + 0.010*\"www\" + 0.009*\"comments\" + 0.006*\"link\" + 0.003*\"amp\" + 0.003*\"thread\" + 0.003*\"bot\"\n",
      "INFO : topic #7 (0.100): 0.024*\"http\" + 0.023*\"com\" + 0.018*\"www\" + 0.018*\"reddit\" + 0.016*\"comments\" + 0.014*\"funny\" + 0.008*\"gifs\" + 0.008*\"yr\" + 0.005*\"like\" + 0.004*\"people\"\n",
      "INFO : topic diff=0.164999, rho=0.447214\n",
      "INFO : PROGRESS: pass 3, at document #2000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #4 (0.100): 0.011*\"people\" + 0.010*\"like\" + 0.006*\"know\" + 0.006*\"think\" + 0.005*\"good\" + 0.005*\"want\" + 0.005*\"time\" + 0.005*\"way\" + 0.004*\"ve\" + 0.004*\"things\"\n",
      "INFO : topic #5 (0.100): 0.004*\"great\" + 0.003*\"bad\" + 0.003*\"think\" + 0.003*\"edit\" + 0.003*\"time\" + 0.002*\"sarcastic\" + 0.002*\"change\" + 0.002*\"word\" + 0.002*\"news\" + 0.002*\"http\"\n",
      "INFO : topic #7 (0.100): 0.028*\"http\" + 0.027*\"com\" + 0.021*\"www\" + 0.021*\"reddit\" + 0.019*\"comments\" + 0.015*\"funny\" + 0.010*\"gifs\" + 0.009*\"yr\" + 0.005*\"like\" + 0.005*\"self\"\n",
      "INFO : topic #0 (0.100): 0.011*\"com\" + 0.011*\"reddit\" + 0.009*\"www\" + 0.008*\"http\" + 0.008*\"bitcoin\" + 0.007*\"comments\" + 0.004*\"link\" + 0.003*\"https\" + 0.003*\"bot\" + 0.003*\"government\"\n",
      "INFO : topic #1 (0.100): 0.005*\"yeah\" + 0.005*\"like\" + 0.005*\"doc\" + 0.004*\"wait\" + 0.004*\"www\" + 0.004*\"reddit\" + 0.004*\"fucking\" + 0.004*\"http\" + 0.004*\"right\" + 0.003*\"going\"\n",
      "INFO : topic diff=0.122762, rho=0.408248\n",
      "INFO : -9.593 per-word bound, 772.6 perplexity estimate based on a held-out corpus of 2000 documents with 31745 words\n",
      "INFO : PROGRESS: pass 3, at document #4000/4000\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #5 (0.100): 0.004*\"bad\" + 0.003*\"great\" + 0.003*\"think\" + 0.003*\"time\" + 0.003*\"edit\" + 0.002*\"sarcastic\" + 0.002*\"training\" + 0.002*\"change\" + 0.002*\"word\" + 0.002*\"news\"\n",
      "INFO : topic #7 (0.100): 0.028*\"http\" + 0.027*\"com\" + 0.021*\"reddit\" + 0.020*\"www\" + 0.018*\"comments\" + 0.015*\"funny\" + 0.009*\"gifs\" + 0.009*\"yr\" + 0.005*\"like\" + 0.004*\"ago\"\n",
      "INFO : topic #6 (0.100): 0.014*\"insurance\" + 0.008*\"time\" + 0.006*\"year\" + 0.006*\"health\" + 0.005*\"care\" + 0.005*\"like\" + 0.005*\"pay\" + 0.004*\"ve\" + 0.003*\"got\" + 0.003*\"people\"\n",
      "INFO : topic #4 (0.100): 0.012*\"people\" + 0.010*\"like\" + 0.006*\"know\" + 0.006*\"think\" + 0.006*\"good\" + 0.005*\"want\" + 0.005*\"time\" + 0.004*\"ve\" + 0.004*\"way\" + 0.004*\"things\"\n",
      "INFO : topic #3 (0.100): 0.036*\"gt\" + 0.017*\"welcome\" + 0.013*\"barrack\" + 0.007*\"meme\" + 0.005*\"com\" + 0.005*\"work\" + 0.005*\"http\" + 0.004*\"reason\" + 0.004*\"imgur\" + 0.004*\"mean\"\n",
      "INFO : topic diff=0.111508, rho=0.408248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.5 s, sys: 1.32 s, total: 36.8 s\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000)\n",
    "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=10, id2word=id2word_comments, passes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.100): 0.013*\"com\" + 0.012*\"reddit\" + 0.011*\"bitcoin\" + 0.010*\"http\" + 0.010*\"www\" + 0.008*\"comments\" + 0.006*\"link\" + 0.003*\"amp\" + 0.003*\"thread\" + 0.003*\"https\"\n",
      "INFO : topic #1 (0.100): 0.006*\"yeah\" + 0.005*\"wait\" + 0.005*\"like\" + 0.004*\"fucking\" + 0.004*\"right\" + 0.003*\"going\" + 0.003*\"www\" + 0.003*\"doc\" + 0.003*\"reddit\" + 0.003*\"http\"\n",
      "INFO : topic #2 (0.100): 0.006*\"like\" + 0.004*\"edit\" + 0.003*\"shit\" + 0.003*\"http\" + 0.003*\"imgur\" + 0.003*\"com\" + 0.002*\"oh\" + 0.002*\"germany\" + 0.002*\"gif\" + 0.002*\"blame\"\n",
      "INFO : topic #3 (0.100): 0.036*\"gt\" + 0.017*\"welcome\" + 0.013*\"barrack\" + 0.007*\"meme\" + 0.005*\"com\" + 0.005*\"work\" + 0.005*\"http\" + 0.004*\"reason\" + 0.004*\"imgur\" + 0.004*\"mean\"\n",
      "INFO : topic #4 (0.100): 0.012*\"people\" + 0.010*\"like\" + 0.006*\"know\" + 0.006*\"think\" + 0.006*\"good\" + 0.005*\"want\" + 0.005*\"time\" + 0.004*\"ve\" + 0.004*\"way\" + 0.004*\"things\"\n",
      "INFO : topic #5 (0.100): 0.004*\"bad\" + 0.003*\"great\" + 0.003*\"think\" + 0.003*\"time\" + 0.003*\"edit\" + 0.002*\"sarcastic\" + 0.002*\"training\" + 0.002*\"change\" + 0.002*\"word\" + 0.002*\"news\"\n",
      "INFO : topic #6 (0.100): 0.014*\"insurance\" + 0.008*\"time\" + 0.006*\"year\" + 0.006*\"health\" + 0.005*\"care\" + 0.005*\"like\" + 0.005*\"pay\" + 0.004*\"ve\" + 0.003*\"got\" + 0.003*\"people\"\n",
      "INFO : topic #7 (0.100): 0.028*\"http\" + 0.027*\"com\" + 0.021*\"reddit\" + 0.020*\"www\" + 0.018*\"comments\" + 0.015*\"funny\" + 0.009*\"gifs\" + 0.009*\"yr\" + 0.005*\"like\" + 0.004*\"ago\"\n",
      "INFO : topic #8 (0.100): 0.005*\"gt\" + 0.004*\"people\" + 0.004*\"wow\" + 0.004*\"like\" + 0.003*\"ve\" + 0.003*\"right\" + 0.003*\"oh\" + 0.003*\"thought\" + 0.003*\"work\" + 0.002*\"government\"\n",
      "INFO : topic #9 (0.100): 0.008*\"people\" + 0.006*\"like\" + 0.005*\"bot\" + 0.004*\"better\" + 0.004*\"got\" + 0.003*\"know\" + 0.003*\"years\" + 0.003*\"country\" + 0.003*\"need\" + 0.003*\"seriously\"\n"
     ]
    }
   ],
   "source": [
    "_ = lda_model.print_topics(-1) # Print most important words in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting document frequencies\n",
      "INFO : PROGRESS: processing document #0\n",
      "INFO : PROGRESS: processing document #10000\n",
      "INFO : PROGRESS: processing document #20000\n",
      "INFO : calculating IDF weights for 25164 documents and 35899 features (332620 matrix non-zeros)\n",
      "INFO : using serial LSI version on this node\n",
      "INFO : updating model with new documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 s, sys: 67 ms, total: 2.02 s\n",
      "Wall time: 2.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : preparing a new chunk of documents\n",
      "INFO : using 100 extra samples and 2 power iterations\n",
      "INFO : 1st phase: constructing (35900, 300) action matrix\n",
      "INFO : orthonormalizing (35900, 300) action matrix\n",
      "INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "INFO : computing the final decomposition\n",
      "INFO : keeping 200 factors (discarding 14.576% of energy spectrum)\n",
      "INFO : processed documents up to #20000\n",
      "INFO : topic #0(19.796): 0.606*\"barrack\" + 0.590*\"welcome\" + 0.529*\"gt\" + 0.041*\"davemills\" + 0.019*\"corncobjohnson\" + 0.016*\"lesweden\" + 0.012*\"bot\" + 0.008*\"says\" + 0.007*\"ftfy\" + 0.006*\"com\"\n",
      "INFO : topic #1(11.256): -0.728*\"bot\" + -0.324*\"yeah\" + -0.116*\"comments\" + -0.112*\"like\" + -0.104*\"subreddit\" + -0.103*\"wait\" + -0.103*\"automoderator\" + -0.103*\"pu\" + -0.103*\"automatically\" + -0.103*\"performed\"\n",
      "INFO : topic #2(10.908): -0.865*\"yeah\" + 0.388*\"bot\" + -0.061*\"oh\" + 0.060*\"automoderator\" + 0.060*\"pu\" + 0.059*\"performed\" + 0.059*\"fr\" + 0.059*\"moderators\" + 0.059*\"compose\" + 0.058*\"concerns\"\n",
      "INFO : topic #3(10.109): -0.377*\"yeah\" + -0.258*\"bot\" + 0.205*\"like\" + 0.200*\"seriously\" + 0.170*\"wait\" + 0.162*\"oh\" + 0.161*\"people\" + 0.147*\"com\" + 0.144*\"http\" + 0.143*\"insurance\"\n",
      "INFO : topic #4(9.438): 0.938*\"biden\" + 0.151*\"joe\" + -0.075*\"comments\" + -0.075*\"automoderator\" + -0.075*\"pu\" + -0.075*\"performed\" + -0.075*\"fr\" + -0.075*\"moderators\" + -0.074*\"compose\" + -0.074*\"concerns\"\n",
      "INFO : preparing a new chunk of documents\n",
      "INFO : using 100 extra samples and 2 power iterations\n",
      "INFO : 1st phase: constructing (35900, 300) action matrix\n",
      "INFO : orthonormalizing (35900, 300) action matrix\n",
      "INFO : 2nd phase: running dense svd on (300, 5164) matrix\n",
      "INFO : computing the final decomposition\n",
      "INFO : keeping 200 factors (discarding 15.375% of energy spectrum)\n",
      "INFO : merging projections: (35900, 200) + (35900, 200)\n",
      "INFO : keeping 200 factors (discarding 5.061% of energy spectrum)\n",
      "INFO : processed documents up to #25164\n",
      "INFO : topic #0(22.295): 0.606*\"barrack\" + 0.593*\"welcome\" + 0.526*\"gt\" + 0.040*\"davemills\" + 0.024*\"corncobjohnson\" + 0.016*\"lesweden\" + 0.013*\"bot\" + 0.008*\"says\" + 0.007*\"ftfy\" + 0.005*\"meme\"\n",
      "INFO : topic #1(12.656): 0.813*\"bot\" + 0.162*\"yeah\" + 0.113*\"wait\" + 0.107*\"comments\" + 0.104*\"like\" + 0.098*\"subreddit\" + 0.097*\"automatically\" + 0.097*\"automoderator\" + 0.096*\"pu\" + 0.096*\"performed\"\n",
      "INFO : topic #2(11.971): 0.843*\"yeah\" + -0.301*\"bot\" + 0.109*\"wait\" + 0.106*\"oh\" + 0.097*\"like\" + 0.081*\"people\" + 0.075*\"right\" + 0.070*\"insurance\" + 0.068*\"know\" + 0.068*\"seriously\"\n",
      "INFO : topic #3(11.278): -0.509*\"yeah\" + 0.240*\"wait\" + -0.240*\"bot\" + 0.183*\"like\" + 0.174*\"oh\" + 0.158*\"seriously\" + 0.153*\"people\" + 0.133*\"insurance\" + 0.132*\"right\" + 0.127*\"com\"\n",
      "INFO : topic #4(10.717): -0.971*\"biden\" + -0.158*\"joe\" + 0.042*\"comments\" + 0.041*\"automoderator\" + 0.041*\"pu\" + 0.040*\"performed\" + 0.040*\"fr\" + 0.040*\"moderators\" + 0.040*\"compose\" + 0.040*\"concerns\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.1 s, sys: 4.02 s, total: 42.1 s\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "# Stack transforms (Latent Semantic Analysis on TFIDF)\n",
    "%time tfidf_model = gensim.models.TfidfModel(mm_corpus, id2word=id2word_comments)\n",
    "%time lsi_model = gensim.models.LsiModel(tfidf_model[mm_corpus], id2word=id2word_comments, num_topics=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0002179994335000898), (1, 0.0029780594872817752), (2, 0.0003011833383196768), (3, 0.001672098518136828), (4, 0.00017254863369103708), (5, -1.9106118729552668e-05), (6, 0.0018322795580526165), (7, 0.00063517318736097976), (8, 0.00062298387650565562), (9, 0.00046996789007005499), (10, 9.0512447555181314e-05), (11, 0.00025975223270507963), (12, -0.0003896589029783597), (13, 0.00065972817648281973), (14, -0.0026826231190943827), (15, 0.0018707953086169547), (16, 1.4688116895818741e-05), (17, -7.0408977442287476e-05), (18, -0.00016534848327647367), (19, 0.0043034298662401535), (20, -0.0011401639865943173), (21, 0.0015329014737278019), (22, -0.0015337308362312484), (23, -0.0021223233416963957), (24, 0.0028289524189796687), (25, 0.0018589414655918726), (26, -0.001069223586516228), (27, 0.0023942593193543495), (28, 0.0032779610753700976), (29, -0.0021313935290583836), (30, 0.00083526876208800762), (31, 0.00028467599416923182), (32, -0.0029140069770172412), (33, -0.00068461216448099558), (34, -0.00015086245975995817), (35, -0.00027286828864657529), (36, 0.0032340601374715346), (37, -0.0021016731180444025), (38, 0.0011344255678056156), (39, -0.00046078409352980136), (40, -0.00054792239984094882), (41, -0.0014464687273440173), (42, 0.0043634395556822209), (43, -0.0028415923601529842), (44, -0.0014540455452194455), (45, -0.0018247202112506664), (46, -0.00067580456476501005), (47, -0.00038535196613817178), (48, -0.0026163073848090115), (49, -0.0039048170501922723), (50, 0.0025284946892715722), (51, -0.0036601087287612389), (52, 0.0045022446600796497), (53, 0.0051311901172145392), (54, -0.0038688494301407999), (55, -0.00063618564775430695), (56, -0.0014389331033060359), (57, 0.00033738034295405662), (58, 0.0015695269012253885), (59, 0.0012801560804987255), (60, -0.0038733790748188652), (61, -0.0028249426725871288), (62, -0.00048260341359101247), (63, -0.00088649213723791288), (64, 0.0058729954687975509), (65, 0.0054284607540647286), (66, -0.0015983826315834663), (67, -0.00073858950071546475), (68, -0.0040432899921400726), (69, 0.0008042891505665316), (70, 0.006054870233809696), (71, -0.0027281126996320224), (72, 4.8644267181651942e-05), (73, -0.0021995283692465861), (74, -0.0018609655621535997), (75, -0.0032334205707183509), (76, -0.0016188744179445158), (77, -0.0030771215496407699), (78, -0.002187934999156549), (79, -0.0033082599113057267), (80, -0.00039812660967804888), (81, -0.00075023894473399185), (82, -0.0014978100443375775), (83, 0.0046706636960152613), (84, 0.0013191491792896273), (85, -0.00052164604628936228), (86, 0.0032997274771531205), (87, -0.0011158042933029164), (88, 0.0028513888159128627), (89, -0.0087596711629452512), (90, -0.0020746060083724968), (91, -0.0045395947999382538), (92, 0.0031165355656568223), (93, 0.0043094315068236549), (94, 0.0028935228705142886), (95, 0.0014876460156883757), (96, 0.0012995658595472269), (97, 0.0029344644411299866), (98, -0.0049305292036050854), (99, -0.0022824488938225263), (100, -0.0029916378247053874), (101, 0.0016655781215604562), (102, 0.016879137769826212), (103, 0.00049851093231765035), (104, -0.00082893645269009023), (105, -0.0031280243841347198), (106, 0.0076521518361607893), (107, 0.0015314959503885412), (108, -0.0020762133406743337), (109, -0.0053382373623395377), (110, 0.0009206331586047254), (111, -0.0023388962203877862), (112, -0.0041318228796943125), (113, -0.0075205802041577981), (114, -0.00042869041507719583), (115, -0.002639649981322871), (116, -0.0025706011540509977), (117, 0.006259935385501781), (118, -0.0024471285031045465), (119, -0.0042346222313920474), (120, -0.0011940128133000932), (121, 0.0041254365918055487), (122, 0.003287608853221356), (123, 0.002528869676234008), (124, -0.0030506949314050009), (125, 0.0022140216476464712), (126, 0.0069149192170887613), (127, 0.0033462529381662631), (128, 0.0026382939713193027), (129, -0.0023304313842396982), (130, -0.0010489732477520217), (131, 0.0025952846145916673), (132, 0.0033458945427878766), (133, -0.0031597382368178462), (134, -0.0045475759805249224), (135, 0.0010776942430876339), (136, 0.0038884264171416866), (137, 0.0062376421988171605), (138, -0.0040074182692973357), (139, 0.0090909431460679639), (140, 0.0067373553658189736), (141, -0.0019793888954094539), (142, 0.002899324138208547), (143, 0.0089404478120592781), (144, 0.0056250288948086011), (145, -0.010505529795445081), (146, 0.006908575257385299), (147, 0.0015543018534871712), (148, 0.0036589837441707365), (149, 0.0053402040591173372), (150, -0.0033161053927167716), (151, 0.002742202199182722), (152, -0.00042623534233079063), (153, 0.0011896036081602381), (154, -0.0024157194042753851), (155, -0.013996976645755959), (156, 0.0012375896591485564), (157, -0.0087214302058991715), (158, -0.0054444734379790541), (159, -0.0085845795108556184), (160, 0.015659458403368326), (161, 0.0088891868273385365), (162, 0.0091556348695297982), (163, -0.0053107578845863306), (164, -0.017964022425545661), (165, 0.012184667381590124), (166, 0.0055292029081303049), (167, -0.012189106273995104), (168, -0.0039869905653692611), (169, -0.0039717089740844017), (170, 0.0070499093150192451), (171, 0.0021212817773667582), (172, -0.0057850757824150093), (173, 0.003023213944539525), (174, 0.006870338253620766), (175, 0.0010517579468260778), (176, 0.0029983540882907783), (177, -0.012381442021772493), (178, -0.0045986276353187014), (179, -0.0032147499973383983), (180, -0.012122819395020462), (181, 0.0045166015358960388), (182, -0.01275047414434268), (183, 0.0005161241763758966), (184, -0.0092325650703690525), (185, -0.0019751366302648747), (186, -0.0020207907204515883), (187, 0.00037562029895945887), (188, 0.00080774128117480171), (189, -0.01435600907380782), (190, -0.0094571923105777611), (191, -0.014301018641775218), (192, -0.0066389307125445099), (193, -0.0022548552920596972), (194, 0.0064887894830355335), (195, 0.0017646567262926693), (196, -0.0028578226283711644), (197, -0.0027075533163354645), (198, 0.0058062281284057167), (199, 0.0065053838440352514)]\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(lsi_model[tfidf_model[mm_corpus]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to ./topic_models/obama_tfidf.mm\n",
      "INFO : saving sparse matrix to ./topic_models/obama_tfidf.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : PROGRESS: saving document #5000\n",
      "INFO : PROGRESS: saving document #6000\n",
      "INFO : PROGRESS: saving document #7000\n",
      "INFO : PROGRESS: saving document #8000\n",
      "INFO : PROGRESS: saving document #9000\n",
      "INFO : PROGRESS: saving document #10000\n",
      "INFO : PROGRESS: saving document #11000\n",
      "INFO : PROGRESS: saving document #12000\n",
      "INFO : PROGRESS: saving document #13000\n",
      "INFO : PROGRESS: saving document #14000\n",
      "INFO : PROGRESS: saving document #15000\n",
      "INFO : PROGRESS: saving document #16000\n",
      "INFO : PROGRESS: saving document #17000\n",
      "INFO : PROGRESS: saving document #18000\n",
      "INFO : PROGRESS: saving document #19000\n",
      "INFO : PROGRESS: saving document #20000\n",
      "INFO : PROGRESS: saving document #21000\n",
      "INFO : PROGRESS: saving document #22000\n",
      "INFO : PROGRESS: saving document #23000\n",
      "INFO : PROGRESS: saving document #24000\n",
      "INFO : PROGRESS: saving document #25000\n",
      "INFO : saved 25164x35900 matrix, density=0.037% (332620/903387600)\n",
      "INFO : saving MmCorpus index to ./topic_models/obama_tfidf.mm.index\n",
      "INFO : storing corpus in Matrix Market format to ./topic_models/obama_lsa.mm\n",
      "INFO : saving sparse matrix to ./topic_models/obama_lsa.mm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.68 s, sys: 432 ms, total: 7.11 s\n",
      "Wall time: 7.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : PROGRESS: saving document #5000\n",
      "INFO : PROGRESS: saving document #6000\n",
      "INFO : PROGRESS: saving document #7000\n",
      "INFO : PROGRESS: saving document #8000\n",
      "INFO : PROGRESS: saving document #9000\n",
      "INFO : PROGRESS: saving document #10000\n",
      "INFO : PROGRESS: saving document #11000\n",
      "INFO : PROGRESS: saving document #12000\n",
      "INFO : PROGRESS: saving document #13000\n",
      "INFO : PROGRESS: saving document #14000\n",
      "INFO : PROGRESS: saving document #15000\n",
      "INFO : PROGRESS: saving document #16000\n",
      "INFO : PROGRESS: saving document #17000\n",
      "INFO : PROGRESS: saving document #18000\n",
      "INFO : PROGRESS: saving document #19000\n",
      "INFO : PROGRESS: saving document #20000\n",
      "INFO : PROGRESS: saving document #21000\n",
      "INFO : PROGRESS: saving document #22000\n",
      "INFO : PROGRESS: saving document #23000\n",
      "INFO : PROGRESS: saving document #24000\n",
      "INFO : PROGRESS: saving document #25000\n",
      "INFO : saved 25164x200 matrix, density=99.638% (5014593/5032800)\n",
      "INFO : saving MmCorpus index to ./topic_models/obama_lsa.mm.index\n",
      "INFO : storing corpus in Matrix Market format to ./topic_models/obama_lda.mm\n",
      "INFO : saving sparse matrix to ./topic_models/obama_lda.mm\n",
      "INFO : PROGRESS: saving document #0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.2 s, sys: 2.95 s, total: 48.2 s\n",
      "Wall time: 50.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : PROGRESS: saving document #5000\n",
      "INFO : PROGRESS: saving document #6000\n",
      "INFO : PROGRESS: saving document #7000\n",
      "INFO : PROGRESS: saving document #8000\n",
      "INFO : PROGRESS: saving document #9000\n",
      "INFO : PROGRESS: saving document #10000\n",
      "INFO : PROGRESS: saving document #11000\n",
      "INFO : PROGRESS: saving document #12000\n",
      "INFO : PROGRESS: saving document #13000\n",
      "INFO : PROGRESS: saving document #14000\n",
      "INFO : PROGRESS: saving document #15000\n",
      "INFO : PROGRESS: saving document #16000\n",
      "INFO : PROGRESS: saving document #17000\n",
      "INFO : PROGRESS: saving document #18000\n",
      "INFO : PROGRESS: saving document #19000\n",
      "INFO : PROGRESS: saving document #20000\n",
      "INFO : PROGRESS: saving document #21000\n",
      "INFO : PROGRESS: saving document #22000\n",
      "INFO : PROGRESS: saving document #23000\n",
      "INFO : PROGRESS: saving document #24000\n",
      "INFO : PROGRESS: saving document #25000\n",
      "INFO : saved 25164x10 matrix, density=80.075% (201500/251640)\n",
      "INFO : saving MmCorpus index to ./topic_models/obama_lda.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.6 s, sys: 1.07 s, total: 34.6 s\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "%time gensim.corpora.MmCorpus.serialize('./topic_models/obama_tfidf.mm', tfidf_model[mm_corpus])\n",
    "%time gensim.corpora.MmCorpus.serialize('./topic_models/obama_lsa.mm', lsi_model[tfidf_model[mm_corpus]])\n",
    "%time gensim.corpora.MmCorpus.serialize('./topic_models/obama_lda.mm', lda_model[mm_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loaded corpus index from ./topic_models/obama_tfidf.mm.index\n",
      "INFO : initializing corpus reader from ./topic_models/obama_tfidf.mm\n",
      "INFO : accepted corpus with 25164 documents, 35900 features, 332620 non-zero entries\n",
      "INFO : loaded corpus index from ./topic_models/obama_lsa.mm.index\n",
      "INFO : initializing corpus reader from ./topic_models/obama_lsa.mm\n",
      "INFO : accepted corpus with 25164 documents, 200 features, 5014593 non-zero entries\n",
      "INFO : loaded corpus index from ./topic_models/obama_lda.mm.index\n",
      "INFO : initializing corpus reader from ./topic_models/obama_lda.mm\n",
      "INFO : accepted corpus with 25164 documents, 10 features, 201500 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(25164 documents, 35900 features, 332620 non-zero entries)\n",
      "MmCorpus(25164 documents, 200 features, 5014593 non-zero entries)\n",
      "MmCorpus(25164 documents, 10 features, 201500 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "tfidf_corpus = gensim.corpora.MmCorpus('./topic_models/obama_tfidf.mm')\n",
    "lsi_corpus = gensim.corpora.MmCorpus('./topic_models/obama_lsa.mm')\n",
    "lda_corpus = gensim.corpora.MmCorpus('./topic_models/obama_lda.mm')\n",
    "print(tfidf_corpus)\n",
    "print(lsi_corpus)\n",
    "print(lda_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'putin', 1), (u'won', 1), (u'government', 1), (u'controls', 1), (u'trump', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"Trump won and now Putin controls the government. Thanks, Obama!\"\n",
    "bow_vector = id2word_comments.doc2bow(tokenize(text))\n",
    "print([(id2word_comments[id], count) for id, count in bow_vector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.016670444548322127), (1, 0.4289209421753305), (2, 0.01666839986469169), (3, 0.016668436573002401), (4, 0.016671940902791098), (5, 0.016668705112579035), (6, 0.016668703670370075), (7, 0.016667957609206887), (8, 0.016670069614534089), (9, 0.43772439992917206)]\n",
      "0.008*\"people\" + 0.006*\"like\" + 0.005*\"bot\" + 0.004*\"better\" + 0.004*\"got\" + 0.003*\"know\" + 0.003*\"years\" + 0.003*\"country\" + 0.003*\"need\" + 0.003*\"seriously\"\n"
     ]
    }
   ],
   "source": [
    "lda_vector = lda_model[bow_vector]\n",
    "print(lda_vector)\n",
    "# print the document's most prominent LDA topic: \n",
    "print(lda_model.print_topic(max(lda_vector, key=lambda item: item[1])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9630, 0.4178697047896979), (12449, 0.30007103723758366), (19947, 0.25953702681478913), (24292, 0.5004726335535591), (34258, 0.6461501042375802)]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vector = tfidf_model[bow_vector]\n",
    "print(tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : saving LdaState object under ./topic_models/lda_obama.model.state, separately None\n",
      "INFO : saved ./topic_models/lda_obama.model.state\n",
      "INFO : saving LdaModel object under ./topic_models/lda_obama.model, separately ['expElogbeta', 'sstats']\n",
      "INFO : not storing attribute id2word\n",
      "INFO : storing np array 'expElogbeta' to ./topic_models/lda_obama.model.expElogbeta.npy\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved ./topic_models/lda_obama.model\n",
      "INFO : saving Projection object under ./topic_models/lsi_obama.model.projection, separately None\n",
      "INFO : saved ./topic_models/lsi_obama.model.projection\n",
      "INFO : saving LsiModel object under ./topic_models/lsi_obama.model, separately None\n",
      "INFO : not storing attribute projection\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : saved ./topic_models/lsi_obama.model\n",
      "INFO : saving TfidfModel object under ./topic_models/tfidf_obama.model, separately None\n",
      "INFO : saved ./topic_models/tfidf_obama.model\n",
      "INFO : saving Dictionary object under ./topic_models/obama.dictionary, separately None\n",
      "INFO : saved ./topic_models/obama.dictionary\n"
     ]
    }
   ],
   "source": [
    "lda_model.save('./topic_models/lda_obama.model')\n",
    "lsi_model.save('./topic_models/lsi_obama.model')\n",
    "tfidf_model.save('./topic_models/tfidf_obama.model')\n",
    "id2word_comments.save('./topic_models/obama.dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'com', u'reddit', u'bitcoin', u'http', u'www', u'comments', u'link', u'amp', u'thread', u'https', u'bot', u'government', u'snowden', u'xb', u'karpeles', u'illegal', u'courtesy', u'jojo', u'mojo', u'bomb', u'anymore', u'youtube', u'stuff', u'says', u'donating', u'summon', u'time', u'watch', u'joseph', u'rules', u'message', u'space', u'hiring', u'kill', u'pony', u'progress', u'np', u'focus', u'noodles', u'apos', u'linked', u'links', u'respect', u'compose', u'gov', u'oh', u'glock', u'vote', u'follow', u'programming'], [u'yeah', u'wait', u'like', u'fucking', u'right', u'going', u'www', u'doc', u'reddit', u'http', u'happened', u'think', u'com', u'time', u'won', u'change', u'oh', u'trying', u'rage', u'watch', u'know', u'okay', u'marty', u'believe', u'wouldn', u'point', u'added', u'sure', u'came', u'comments', u'people', u'app', u'freedom', u'look', u'government', u'away', u'work', u'great', u'amazon', u'th', u'twice', u'woman', u'thing', u'youtube', u'wasn', u'maybe', u'sides', u'gun', u'job', u'dropped'], [u'like', u'edit', u'shit', u'http', u'imgur', u'com', u'oh', u'germany', u'gif', u'blame', u'people', u'blah', u'hate', u'got', u'president', u'fucking', u'la', u'laughed', u'old', u'image', u'realize', u'works', u'ebola', u'dr', u'god', u'banks', u'xd', u'usa', u'thread', u'amp', u'reference', u'dream', u'tl', u'le', u'whoa', u'wrong', u'new', u'bunch', u'da', u'type', u'small', u'yup', u'story', u'clinton', u'sigh', u'twist', u'defend', u'lost', u'helps', u'racist'], [u'gt', u'welcome', u'barrack', u'meme', u'com', u'work', u'http', u'reason', u'imgur', u'mean', u'like', u'biden', u'says', u'post', u'original', u'case', u'school', u'time', u'blocked', u'griff', u'title', u'know', u'create', u'marty', u'link', u'fucking', u'linked', u'template', u'think', u'version', u'unavailable', u'memegen', u'right', u'yeah', u'people', u'upvote', u'anymore', u'good', u'look', u'progressives', u'joe', u'damn', u'away', u'oh', u'guy', u'obligatory', u'need', u'add', u'season', u'seriously'], [u'people', u'like', u'know', u'think', u'good', u'want', u'time', u've', u'way', u'things', u'right', u'got', u'government', u'actually', u'money', u'going', u'said', u'healthcare', u'joke', u'edit', u'bad', u'insurance', u'pretty', u'gt', u'saying', u'll', u'let', u'years', u'pay', u'getting', u'need', u'year', u'shit', u'fault', u'free', u'president', u'use', u'thing', u'state', u'blame', u'making', u'real', u'fuck', u'went', u'great', u'lot', u'sure', u'better', u'look', u'tax'], [u'bad', u'great', u'think', u'time', u'edit', u'sarcastic', u'training', u'change', u'word', u'news', u'proud', u'll', u'obamabot', u'asia', u'chaos', u'ugh', u'hey', u'thing', u'died', u'http', u'quality', u'acne', u'words', u'haha', u'pop', u'com', u'glad', u'inb', u'diagnosed', u'shock', u'providing', u'throw', u'department', u'ah', u'china', u'oh', u'saudi', u'genuine', u'aka', u'like', u'reddit', u'going', u'influenced', u'confirm', u'disagree', u'administration', u'sight', u'rifles', u'executive', u'edges'], [u'insurance', u'time', u'year', u'health', u'care', u'like', u'pay', u've', u'got', u'people', u'fucking', u'thing', u'obamacare', u'shit', u'month', u'actually', u'fuck', u'free', u'covered', u'life', u'working', u'going', u'said', u'coverage', u'school', u'll', u'gets', u'day', u'cost', u'deductible', u'money', u'costs', u'getting', u'hard', u'medical', u'work', u'couldn', u'eat', u'robot', u'paid', u'years', u'war', u'reason', u'high', u'went', u'nsa', u'think', u'try', u'great', u'major'], [u'http', u'com', u'reddit', u'www', u'comments', u'funny', u'gifs', u'yr', u'like', u'ago', u'self', u'people', u'gif', u'mos', u'imgur', u'time', u'president', u'thanksobama', u'jpg', u'day', u'new', u'world', u'similar', u'fuck', u'trying', u'submitted', u'yrs', u'year', u'black', u'minutes', u'kb', u'let', u'player', u'walls', u'saying', u'post', u'point', u'head', u'making', u'points', u'goldreply', u'prices', u'got', u'think', u'cool', u'gifsound', u'wrong', u'check', u'right', u'god'], [u'gt', u'people', u'wow', u'like', u've', u'right', u'oh', u'thought', u'work', u'government', u'going', u'global', u'thank', u'come', u'country', u'president', u'comment', u'warming', u'sarcasm', u'man', u'yeah', u'think', u'need', u'literally', u'll', u'goes', u'good', u'cause', u'yea', u'ozone', u'got', u'problems', u'wait', u'life', u'illegal', u'cfcs', u'quickly', u'guys', u'sit', u'freedom', u'completely', u'case', u'realize', u'buy', u'outside', u'know', u'sure', u'god', u'economic', u'milk'], [u'people', u'like', u'bot', u'better', u'got', u'know', u'years', u'country', u'need', u'seriously', u'think', u'way', u'guy', u'right', u'love', u'obamacare', u'going', u'kids', u'wrong', u've', u'damn', u'action', u'came', u'want', u'care', u'war', u'work', u'school', u'good', u'gt', u'new', u'questions', u'person', u'comments', u'message', u'jobs', u'god', u'automatically', u'subreddit', u'bad', u'thing', u'feel', u'wasn', u'yes', u'help', u'small', u'time', u'great', u'lot', u'place']]\n"
     ]
    }
   ],
   "source": [
    "# select top 50 words for each of the 20 LDA topics\n",
    "top_words = [[word for word, _ in lda_model.show_topic(topicno, topn=50)] for topicno in range(lda_model.num_topics)]\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you spot the misplaced word in each topic?\n",
      "0: com reddit bitcoin dream www comments link amp thread https\n",
      "1: yeah add like fucking right going www doc reddit http\n",
      "2: like edit shit http china com oh germany gif blame\n",
      "3: gt welcome barrack meme wasn work http reason imgur mean\n",
      "4: people like obamacare think good want time ve way things\n",
      "5: bad mojo think time edit sarcastic training change word news\n",
      "6: insurance time year old care like pay ve got people\n",
      "7: http com reddit www comments funny gifs yr like medical\n",
      "8: gt imgur wow like ve right oh thought work government\n",
      "9: genuine like bot better got know years country need seriously\n"
     ]
    }
   ],
   "source": [
    "# get all top 50 words in all 20 topics, as one large set\n",
    "all_words = set(itertools.chain.from_iterable(top_words))\n",
    "\n",
    "print(\"Can you spot the misplaced word in each topic?\")\n",
    "\n",
    "# for each topic, replace a word at a different index, to make it more interesting\n",
    "replace_index = np.random.randint(0, 10, lda_model.num_topics)\n",
    "\n",
    "replacements = []\n",
    "for topicno, words in enumerate(top_words):\n",
    "    other_words = all_words.difference(words)\n",
    "    replacement = np.random.choice(list(other_words))\n",
    "    replacements.append((words[replace_index[topicno]], replacement))\n",
    "    words[replace_index[topicno]] = replacement\n",
    "    print(\"%i: %s\" % (topicno, ' '.join(words[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual replacements were:\n",
      "[(0, (u'http', u'dream')), (1, (u'wait', u'add')), (2, (u'imgur', u'china')), (3, (u'com', u'wasn')), (4, (u'know', u'obamacare')), (5, (u'great', u'mojo')), (6, (u'health', u'old')), (7, (u'ago', u'medical')), (8, (u'people', u'imgur')), (9, (u'people', u'genuine'))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual replacements were:\")\n",
    "print(list(enumerate(replacements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA results:\n",
      "average cosine similarity between corresponding parts (higher is better):\n",
      "0.464171597579\n",
      "average cosine similarity between 10,000 random parts (lower is better):\n",
      "0.376446967811\n",
      "LSI results:\n",
      "average cosine similarity between corresponding parts (higher is better):\n",
      "0.0813486492936\n",
      "average cosine similarity between 10,000 random parts (lower is better):\n",
      "0.0158405848284\n"
     ]
    }
   ],
   "source": [
    "# Really didn't work because of very small size of documents\n",
    "\n",
    "# evaluate on 1k documents **not** used in LDA training\n",
    "doc_stream = (tokens for _, tokens in iter_comment_dump('./all_years/obama-test.json'))  # generator\n",
    "test_docs = list(doc_stream)\n",
    "def intra_inter(model, test_docs, num_pairs=10000):\n",
    "    # split each test document into two halves and compute topics for each half\n",
    "    part1 = [model[id2word_comments.doc2bow(tokens[: len(tokens) / 2])] for tokens in test_docs]\n",
    "    part2 = [model[id2word_comments.doc2bow(tokens[len(tokens) / 2 :])] for tokens in test_docs]\n",
    "\n",
    "    # print computed similarities (uses cossim)\n",
    "    print(\"average cosine similarity between corresponding parts (higher is better):\")\n",
    "    print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "\n",
    "    random_pairs = np.random.randint(0, len(test_docs), size=(num_pairs, 2))\n",
    "    print(\"average cosine similarity between 10,000 random parts (lower is better):\")    \n",
    "    print(np.mean([gensim.matutils.cossim(part1[i[0]], part2[i[1]]) for i in random_pairs]))\n",
    "\n",
    "print(\"LDA results:\")\n",
    "intra_inter(lda_model, test_docs)\n",
    "print(\"LSI results:\")\n",
    "intra_inter(lsi_model, test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
